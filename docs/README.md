# Brainly â€” System Documentation

> **Personal knowledge management with AI intelligence.**
> Save URLs from anywhere, get automatic metadata extraction, semantic search, and AI-powered insights from your saved knowledge.

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Tech Stack](#2-tech-stack)
3. [Architecture](#3-architecture)
4. [Database Schema](#4-database-schema)
5. [API Reference](#5-api-reference)
6. [Content Enrichment Pipeline](#6-content-enrichment-pipeline)
7. [Vector Embeddings & Semantic Search](#7-vector-embeddings--semantic-search)
8. [AI Module System](#8-ai-module-system)
9. [Authentication](#9-authentication)
10. [Local Development Setup](#10-local-development-setup)
11. [Configuration Reference](#11-configuration-reference)
12. [Production Readiness Assessment](#12-production-readiness-assessment)
13. [Roadmap](#13-roadmap)

---

## 1. Project Overview

Brainly is a second-brain application. The core loop:

1. **Save** â€” Paste a URL (YouTube, GitHub, Twitter/X, Medium, Instagram, Notion, or any web article)
2. **Enrich** â€” Background service automatically extracts transcripts, READMEs, article text, author info, thumbnails
3. **Organize** â€” Tag content, search it, share your brain publicly
4. **AI (Phase 1+)** â€” Semantic search, auto-summarization, auto-tagging, RAG chat with your knowledge base

### Repository Structure

```
brainly-monorepo/
â”œâ”€â”€ backend/                  # Express + TypeScript backend (this codebase)
â”œâ”€â”€ frontend/                 # React frontend (separate)
â””â”€â”€ docs/                     # All documentation (you are here)
    â”œâ”€â”€ README.md             # This file â€” system overview
    â”œâ”€â”€ api-reference.md      # Detailed API endpoint docs
    â”œâ”€â”€ database-schema.md    # Schema, indexes, constraints
    â”œâ”€â”€ architecture.md       # System design, data flows, decisions
    â”œâ”€â”€ production-guide.md   # Production readiness + scaling gaps
    â”œâ”€â”€ engineering_wiki/     # Deep-dive technical articles
    â””â”€â”€ plans/                # Implementation plans (historical)
```

---

## 2. Tech Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| Runtime | Node.js + TypeScript | TS 5.8 | Type-safe backend |
| Web framework | Express | 4.x | HTTP server + routing |
| Database | PostgreSQL 17 | timescaledb-ha image | Primary datastore |
| ORM | Drizzle ORM | 0.45 | Type-safe queries + migrations |
| Vector/AI layer | pgai (Timescale) | latest | Declarative embedding sync |
| Vector storage | pgvector | (built into timescaledb-ha) | Vector type + similarity ops |
| LLM abstraction | Vercel AI SDK | 6.x | Provider-agnostic LLM calls |
| LLM providers | OpenAI / Anthropic | â€” | Swap via env var |
| Embedding model | text-embedding-3-small | OpenAI | 1536-dim content embeddings |
| Auth | JWT + bcrypt + Google OAuth | â€” | 7-day tokens, bcrypt cost 10 |
| Security | Helmet + express-rate-limit + Zod | â€” | Headers, rate limits, validation |
| Logging | Pino | 10.x | Structured JSON logs |
| Containerization | Docker Compose | â€” | Local dev infrastructure |

---

## 3. Architecture

### High-Level System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (React)                           â”‚
â”‚   Save URLs â”‚ Browse content â”‚ Tag â”‚ Search â”‚ Share â”‚ Chat (P2)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP (JSON)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Express HTTP Server                             â”‚
â”‚   Auth routes â”‚ Content routes â”‚ Tag routes â”‚ Brain sharing       â”‚
â”‚                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                Enrichment Service                        â”‚    â”‚
â”‚   â”‚   setInterval(30s) â†’ DISTINCT ON (user_id) batch        â”‚    â”‚
â”‚   â”‚   â†’ atomic claim â†’ extractor â†’ save metadata            â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚              Vercel AI SDK (Phase 1+)                    â”‚    â”‚
â”‚   â”‚   LLM_PROVIDER=openai â†’ GPT-4o-mini (default)           â”‚    â”‚
â”‚   â”‚   LLM_PROVIDER=anthropic â†’ Claude Sonnet (default)      â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ pg.Pool (max 20 connections)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL 17 (timescaledb-ha)                       â”‚
â”‚                                                                   â”‚
â”‚   users â”‚ contents â”‚ tags â”‚ content_tags â”‚ share_links           â”‚
â”‚                                                                   â”‚
â”‚   contents_embedding_store  â† auto-created by pgai               â”‚
â”‚   (VECTOR(1536) per chunk)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ polls DB queue
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           pgai Vectorizer Worker (Docker service)                 â”‚
â”‚   Detects full_text changes â†’ chunks â†’ OpenAI embed â†’ stores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Architectural Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Single database | PostgreSQL + pgvector | Vectors live next to relational data â€” no sync layer needed |
| ORM | Drizzle (not Prisma) | Allows raw SQL for `DISTINCT ON`, pgai functions; TypeScript-first |
| Embedding management | pgai vectorizer worker | Zero app code for embeddings â€” declarative, automatic sync |
| LLM abstraction | Vercel AI SDK | Single interface for OpenAI/Anthropic/Gemini/Ollama |
| Enrichment architecture | In-process polling (Phase 0) | Simple for MVP; upgradeable to BullMQ+Redis when scaling |
| Frontend compat | `withMongoId()` transform | Returns `_id` (MongoDB convention) without touching frontend code |
| Race condition handling | insert-then-catch-23505 | Eliminates TOCTOU window on signup, tag creation, share links |

---

## 4. Database Schema

### Tables Overview

```
users (auth identity)
  â””â”€â”€< contents (saved URLs, one-to-many)
  â””â”€â”€< tags (user-defined labels, one-to-many)
  â””â”€â”€< share_links (max one per user)

contents >â”€â”€< tags  (via content_tags junction table)

contents_embedding_store  â† auto-created by pgai
  â””â”€â”€ FK â†’ contents.id CASCADE
```

### `users`

| Column | Type | Constraints | Notes |
|--------|------|-------------|-------|
| `id` | `uuid` | PK, default `gen_random_uuid()` | |
| `username` | `varchar(50)` | UNIQUE, nullable | Null for Google-only accounts |
| `email` | `varchar(255)` | UNIQUE, nullable | Set for Google OAuth accounts |
| `password` | `text` | nullable | bcrypt hash; null for Google accounts |
| `google_id` | `text` | UNIQUE, nullable | Google subject ID |
| `profile_picture` | `text` | nullable | URL from Google OAuth |
| `auth_provider` | `varchar(20)` | NOT NULL, default `'local'` | `'local'` or `'google'` |
| `created_at` | `timestamptz` | NOT NULL, default `now()` | |

### `tags`

| Column | Type | Constraints | Notes |
|--------|------|-------------|-------|
| `id` | `uuid` | PK | |
| `name` | `varchar(100)` | NOT NULL | Stored lowercase, trimmed |
| `user_id` | `uuid` | NOT NULL, FK â†’ users.id CASCADE | |
| `created_at` | `timestamptz` | NOT NULL | |

**Indexes:** `UNIQUE(name, user_id)` â€” same tag name can exist for different users.

### `contents`

| Column | Type | Notes |
|--------|------|-------|
| `id` | `uuid` PK | |
| `title` | `varchar(500)` NOT NULL | User-provided title |
| `link` | `text` NOT NULL | Original URL |
| `content_id` | `text` | Platform-specific ID (YouTube video ID, etc.) |
| `type` | `varchar(30)` | `youtube`, `github`, `twitter`, `medium`, `instagram`, `notion`, `generic` |
| `user_id` | `uuid` FK â†’ users CASCADE | |
| `enrichment_status` | `varchar(20)` | `pending` â†’ `processing` â†’ `enriched` / `failed` / `skipped` |
| `enrichment_error` | `text` | Last error message if failed |
| `enrichment_retries` | `integer` default 0 | Max 3 retries |
| `enriched_at` | `timestamptz` | When enrichment completed |
| `meta_title` | `text` | Extracted page/video title |
| `meta_description` | `text` | Extracted description |
| `meta_author` | `text` | Author name |
| `meta_author_url` | `text` | Author profile URL |
| `meta_thumbnail` | `text` | Thumbnail image URL |
| `meta_published_at` | `timestamptz` | Original publish date |
| `meta_tags` | `jsonb` | `string[]` â€” provider-extracted tags |
| `meta_language` | `varchar(10)` | `en`, `es`, etc. |
| `full_text` | `text` | **Embedding source** â€” article body, video transcript, README |
| `full_text_type` | `varchar(20)` | `transcript`, `article`, `readme`, etc. |
| `transcript_segments` | `jsonb` | `[{text, start, duration}]` for YouTube |
| `provider_data` | `jsonb` | Raw provider-specific data |
| `extracted_at` | `timestamptz` | When extraction ran |
| `extractor_version` | `varchar(20)` | Extractor version for cache invalidation |
| `summary` | `text` | **AI field (Phase 1)** â€” LLM-generated summary |
| `ai_tags` | `jsonb` | **AI field (Phase 1)** â€” LLM-suggested tags |
| `created_at` | `timestamptz` NOT NULL | |
| `updated_at` | `timestamptz` NOT NULL | Managed by PostgreSQL trigger â€” never set manually |

**Indexes:**
- `idx_contents_user_created` on `(user_id, created_at)` â€” feeds `GET /content`
- `idx_contents_enrichment` on `(enrichment_status, created_at)` â€” feeds enrichment batch query

**Trigger:** `contents_updated_at` â€” `BEFORE UPDATE`, sets `updated_at = NOW()` automatically.

### `content_tags` (junction)

| Column | Type | Notes |
|--------|------|-------|
| `content_id` | `uuid` | FK â†’ contents.id CASCADE |
| `tag_id` | `uuid` | FK â†’ tags.id CASCADE |

**Primary key:** `(content_id, tag_id)` â€” prevents duplicate tag assignments.

### `share_links`

| Column | Type | Notes |
|--------|------|-------|
| `id` | `uuid` PK | |
| `hash` | `varchar(20)` | UNIQUE â€” 10 random hex chars |
| `user_id` | `uuid` | UNIQUE, FK â†’ users.id CASCADE â€” **one link per user** |

### `contents_embedding_store` (auto-created by pgai)

> This table is never defined by application code. pgai creates it when `setup:pgai` runs.

| Column | Type | Notes |
|--------|------|-------|
| `embedding_uuid` | `uuid` PK | |
| `id` | `uuid` | FK â†’ contents.id CASCADE â€” source row |
| `chunk_seq` | `integer` | 0, 1, 2â€¦ for each chunk of `full_text` |
| `chunk` | `text` | The formatted text chunk that was embedded |
| `embedding` | `vector(1536)` | OpenAI text-embedding-3-small output |

---

## 5. API Reference

### Authentication

All protected endpoints require:
```
Authorization: Bearer <jwt_token>
```

### Auth Endpoints

#### `POST /api/v1/signup`
Rate limited: 10 req / 15 min.

**Request:**
```json
{ "username": "johndoe", "password": "secret123" }
```
- `username`: 3â€“30 chars, `[a-zA-Z0-9_]` only
- `password`: 6â€“100 chars

**Responses:**
| Status | Body | Condition |
|--------|------|-----------|
| 201 | `{ "message": "Account created successfully" }` | Success |
| 400 | `{ "message": "<validation error>" }` | Invalid input |
| 409 | `{ "message": "Username already exists" }` | Duplicate username |

---

#### `POST /api/v1/signin`
Rate limited: 10 req / 15 min.

**Request:**
```json
{ "username": "johndoe", "password": "secret123" }
```

**Responses:**
| Status | Body | Condition |
|--------|------|-----------|
| 200 | `{ "token": "<jwt>" }` | Success â€” 7-day JWT |
| 400 | `{ "message": "Invalid credentials" }` | User not found |
| 400 | `{ "message": "This account uses Google sign-in..." }` | Google-only account |
| 403 | `{ "message": "Incorrect Credentials" }` | Wrong password |

---

#### `POST /api/v1/auth/google`
Rate limited: 10 req / 15 min.

**Request:**
```json
{ "credential": "<google_id_token>" }
```

**Behavior:**
- Verifies Google ID token
- If user exists by `google_id` or `email` â†’ return JWT
- If new user â†’ insert (race-safe with 23505 catch + re-fetch) â†’ return JWT
- If existing local account with same email â†’ links Google ID to account

**Responses:**
| Status | Body |
|--------|------|
| 200 | `{ "token": "<jwt>" }` |
| 400 | `{ "message": "Google credential is required" }` |
| 401 | `{ "message": "Google authentication failed" }` |
| 503 | `{ "message": "Google authentication is not configured" }` |

---

#### `GET /api/v1/me` ğŸ”’
**Response:**
```json
{
  "user": {
    "id": "uuid",
    "username": "johndoe",
    "email": "john@example.com",
    "profilePicture": "https://...",
    "authProvider": "local"
  }
}
```

---

### Content Endpoints

#### `POST /api/v1/content` ğŸ”’
Rate limited: 30 req / 15 min.

**Request:**
```json
{
  "title": "Great YouTube video",
  "link": "https://youtube.com/watch?v=dQw4w9WgXcQ",
  "tags": ["uuid-tag-1", "uuid-tag-2"]
}
```

**Behavior:**
- Validates URL format and detects provider type
- Inserts content row with `enrichmentStatus='pending'`
- Assigns tags (only tags owned by user are accepted)
- Enrichment service picks it up within 30 seconds

**Response (201):**
```json
{
  "message": "Content created successfully",
  "content": {
    "_id": "uuid",
    "title": "Great YouTube video",
    "link": "https://youtube.com/watch?v=dQw4w9WgXcQ",
    "type": "youtube",
    "contentId": "dQw4w9WgXcQ",
    "tags": [{ "_id": "uuid", "name": "music" }],
    "displayName": "YouTube",
    "embedUrl": "https://www.youtube.com/embed/dQw4w9WgXcQ",
    "canonicalUrl": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
    "canEmbed": true
  }
}
```

---

#### `GET /api/v1/content` ğŸ”’
Returns all content for the authenticated user, with populated tags.

**Response:**
```json
{
  "content": [
    {
      "_id": "uuid",
      "title": "Great YouTube video",
      "link": "https://...",
      "type": "youtube",
      "contentId": "dQw4w9WgXcQ",
      "userId": "uuid",
      "createdAt": "2026-02-25T10:00:00Z",
      "tags": [{ "_id": "uuid", "name": "music" }]
    }
  ]
}
```

---

#### `DELETE /api/v1/content` ğŸ”’
**Request:**
```json
{ "contentId": "uuid" }
```
Deletes content + all `content_tags` rows (cascade). Only deletes if owned by user.

---

#### `POST /api/v1/content/validate` ğŸ”’
**Request:** `{ "link": "https://github.com/user/repo" }`

**Response:**
```json
{
  "valid": true,
  "type": "github",
  "displayName": "GitHub",
  "contentId": "user/repo",
  "embedUrl": null,
  "canonicalUrl": "https://github.com/user/repo",
  "canEmbed": false,
  "embedType": null
}
```

---

#### `GET /api/v1/content/providers`
No auth required.

Returns list of supported providers with metadata (name, icon, supported embed types).

---

#### `PUT /api/v1/content/:contentId/tags` ğŸ”’
**Request:** `{ "tags": ["uuid-tag-1", "uuid-tag-2"] }`

Replaces all tags on the content item. Runs as a **database transaction** (atomic delete + insert). Tags not owned by the user are silently ignored.

---

### Tag Endpoints

#### `GET /api/v1/tags` ğŸ”’
Returns all tags for the authenticated user, ordered alphabetically.

```json
{ "tags": [{ "_id": "uuid", "name": "ai", "userId": "uuid", "createdAt": "..." }] }
```

---

#### `POST /api/v1/tags` ğŸ”’
**Request:** `{ "name": "machine learning" }`

- Name is trimmed and lowercased before storage
- Max 50 characters
- Duplicate names return 409 (race-safe via insert-catch-23505)

**Responses:**
| Status | Body |
|--------|------|
| 201 | `{ "message": "Tag created successfully", "tag": { "_id": "...", "name": "machine learning" } }` |
| 409 | `{ "message": "Tag already exists", "tag": { ... } }` |

---

#### `DELETE /api/v1/tags/:tagId` ğŸ”’
Deletes tag and automatically removes all `content_tags` associations (FK cascade).

---

### Brain Sharing Endpoints

#### `POST /api/v1/brain/share` ğŸ”’
**Enable sharing:** `{ "share": true }` â†’ Returns `{ "hash": "a1b2c3d4e5" }`

**Disable sharing:** `{ "share": false }` â†’ Returns `{ "message": "removed link" }`

One share link per user maximum. Idempotent â€” calling with `share: true` twice returns the same hash.

---

#### `GET /api/v1/brain/:shareLink`
No auth required â€” public endpoint.

**Response:**
```json
{
  "username": "johndoe",
  "content": [
    { "_id": "uuid", "title": "...", "link": "...", "type": "youtube", "contentId": "..." }
  ]
}
```

---

## 6. Content Enrichment Pipeline

The enrichment service runs as a background loop inside the main Node.js process. It polls for pending content and runs the appropriate extractor.

### Lifecycle States

```
 [saved by user]
       â”‚
       â–¼
   pending  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                    â”‚ (retry, retries < maxRetries)
       â–¼                                    â”‚
  processing  â”€â”€â”€â”€ extractor fails â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
       â”‚                                    â”‚
       â”œâ”€â”€â”€ extractor succeeds â”€â”€â”€â”€â”€â”€â–º enriched
       â”‚
       â”œâ”€â”€â”€ no extractor for type â”€â”€â–º skipped
       â”‚
       â””â”€â”€â”€ retries exhausted â”€â”€â”€â”€â”€â”€â–º failed
```

### Batch Selection â€” Per-User Fairness

The enrichment batch query uses `DISTINCT ON (user_id)` to ensure fairness across users. A single user with 1000 saved links cannot monopolize the queue:

```sql
SELECT DISTINCT ON (user_id)
    id, type, link, content_id, title, user_id, enrichment_retries, created_at
FROM contents
WHERE enrichment_status = 'pending'
  AND (
    enrichment_retries = 0
    OR (
        enrichment_retries > 0
        AND enrichment_retries < 3
        AND updated_at < NOW() - INTERVAL '60 seconds'
    )
  )
ORDER BY user_id, created_at ASC
LIMIT 5
```

**Result:** At most 1 item per user, up to 5 users per batch, processed with concurrency of 3.

### Atomic Claim â€” Prevents Duplicate Processing

```sql
UPDATE contents
SET enrichment_status = 'processing'
WHERE id = $1 AND enrichment_status = 'pending'
RETURNING *
```

If this returns no row, another process already claimed it. The current process skips it silently.

### Extractors

| Type | Extractor | API Used | What it extracts |
|------|-----------|---------|-----------------|
| `youtube` | `youtube.extractor.ts` | YouTube Innertube (no key req'd) | Title, description, chapters, full transcript, author, publish date |
| `github` | `github.extractor.ts` | GitHub REST API (`GITHUB_TOKEN`) | README full text, description, topics, stars, language |
| `twitter` | `twitter.extractor.ts` | Twitter oEmbed (`TWITTER_BEARER_TOKEN`) | Tweet text, author |
| `medium` | `medium.extractor.ts` | HTML scrape | Article full text, author, tags |
| `instagram` | `instagram.extractor.ts` | Instagram Basic Display (`INSTAGRAM_APP_ID`) | Caption |
| `generic` | `generic.extractor.ts` | @extractus/article-extractor | Article text from any URL |

All extractors degrade gracefully â€” missing API keys cause `enrichmentStatus='skipped'`.

### Crash Recovery

On startup, the enrichment service resets all rows stuck in `'processing'` back to `'pending'`. This handles the case where the server crashed mid-enrichment.

---

## 7. Vector Embeddings & Semantic Search

### Storage Architecture

Vectors are stored **inside PostgreSQL**, not in a separate vector database. pgvector (bundled in `timescaledb-ha:pg17`) provides the `VECTOR` type and similarity operators.

```
PostgreSQL
â”œâ”€â”€ contents                     â† your saved content
â””â”€â”€ contents_embedding_store     â† AUTO-CREATED by pgai
    â”œâ”€â”€ id (FK â†’ contents.id)
    â”œâ”€â”€ chunk_seq (0, 1, 2â€¦)
    â”œâ”€â”€ chunk (text segment, with title/type prefix)
    â””â”€â”€ embedding VECTOR(1536)   â† OpenAI text-embedding-3-small
```

### Embedding Configuration

| Setting | Value |
|---------|-------|
| Model | `text-embedding-3-small` |
| Dimensions | 1536 |
| Chunk size | 1000 characters |
| Chunk overlap | 200 characters |
| Format | `"Title: {title}\nType: {type}\n\n{chunk}"` |
| Trigger | Automatic on `full_text` insert/update |

### How Embeddings Are Generated

The `pgai-vectorizer-worker` Docker service watches a PostgreSQL queue. When `full_text` is populated by the enrichment service, the worker:

1. Reads the new/updated `full_text`
2. Splits it into chunks (1000 chars, 200 overlap)
3. Prefixes each chunk with title + type metadata
4. Calls `OpenAI /embeddings` API
5. Stores 1536-dim vectors in `contents_embedding_store`

**No application code is involved in this process.**

### Semantic Search Query (Phase 1 implementation)

```sql
-- Find content semantically similar to the user's query
SELECT
    c.id,
    c.title,
    c.link,
    c.type,
    e.chunk,
    1 - (e.embedding <=> ai.openai_embed('text-embedding-3-small', $query)) AS similarity
FROM contents_embedding_store e
JOIN contents c ON c.id = e.id
WHERE c.user_id = $userId
ORDER BY similarity DESC
LIMIT 10;
```

The `<=>` operator is cosine distance (from pgvector). `ai.openai_embed()` is an SQL function that calls OpenAI from inside PostgreSQL â€” no application-layer embedding call needed.

---

## 8. AI Module System

All AI modules live in `backend/src/ai/` and follow a portable, dependency-injected design pattern. Each module can be dropped into any Node.js codebase.

### LLM Client

`src/ai/shared/llm-client.ts` wraps the Vercel AI SDK:

```typescript
import { getDefaultModel, getModel } from './ai/shared/llm-client';

const model = getDefaultModel();          // reads LLM_PROVIDER env var
const model = getModel('anthropic');      // explicit provider
const model = getModel('openai', 'gpt-4o'); // explicit model
```

| `LLM_PROVIDER` | Default model |
|----------------|--------------|
| `openai` | `gpt-4o-mini` |
| `anthropic` | `claude-sonnet-4-6-20250514` |

Providers are lazily initialized â€” only created on first use. Missing API keys throw on first call, not at startup.

### Module Directory (Current + Planned)

```
src/ai/
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ llm-client.ts     âœ… Built â€” provider-agnostic LLM wrapper
â”‚   â””â”€â”€ types.ts          âœ… Built â€” shared interfaces
â”œâ”€â”€ summarizer/           ğŸ”² Phase 1 â€” auto-summarize on enrichment
â”œâ”€â”€ search/               ğŸ”² Phase 1 â€” semantic search via pgai
â”œâ”€â”€ tagger/               ğŸ”² Phase 1 â€” auto-tag suggestions
â””â”€â”€ rag/                  ğŸ”² Phase 2 â€” chat with your brain
```

### Shared Types (`src/ai/shared/types.ts`)

```typescript
interface Summary        { text: string; style: 'brief'|'detailed'|'bullet-points'; modelUsed: string; }
interface SearchResult   { contentId, title, link, type, relevance, snippet }
interface TagSuggestion  { tag: string; confidence: number; }
interface ChatMessage    { role: 'user'|'assistant'; content: string; }
interface ChatSource     { contentId, title, link, relevance }
interface ChatResponse   { text: string; sources: ChatSource[]; }
```

---

## 9. Authentication

### JWT Flow

```
POST /api/v1/signup or /signin
          â”‚
          â–¼
    jwt.sign({ id: user.id }, JWT_SECRET, { expiresIn: '7d' })
          â”‚
          â–¼
    Client stores token

Protected request:
    Authorization: Bearer <token>
          â”‚
          â–¼
    userMiddleware: jwt.verify(token, JWT_SECRET)
          â”‚
          â–¼
    req.userId = decoded.id   (string, non-optional)
          â”‚
          â–¼
    Every DB query filters by userId
```

### Google OAuth Flow

```
Frontend (Google One Tap / OAuth popup)
    â†’ receives Google ID token
    â†’ POST /api/v1/auth/google { credential: "<id_token>" }

Backend:
    1. googleClient.verifyIdToken(credential)
    2. Extract { sub: googleId, email, picture }
    3. SELECT user WHERE google_id = $googleId OR email = $email
    4a. User found â†’ return JWT
    4b. No user â†’ INSERT (catch 23505 race â†’ re-fetch) â†’ return JWT
    4c. User found but no googleId â†’ link Google to existing account
```

### Security Details

| Concern | Implementation |
|---------|---------------|
| Password storage | bcrypt, cost factor 10 |
| Duplicate username | `INSERT` then catch error code `23505` (no TOCTOU window) |
| Cross-user data | Every query has `AND user_id = $userId` from JWT |
| Rate limiting | Auth: 10/15min, Content creation: 30/15min, Global: 100/15min |
| Request headers | Helmet (XSS, HSTS, no-sniff, etc.) |
| Body parsing | No size limit currently (1mb limit recommended for production) |
| SQL injection | Drizzle parameterized queries; raw SQL uses `sql` template tag |

---

## 10. Local Development Setup

### Prerequisites
- Docker + Docker Compose
- Node.js 20+
- npm

### Step-by-Step

```bash
# 1. Clone and install
cd backend
npm install

# 2. Configure environment
cp .env.example .env
# Edit .env â€” at minimum set:
#   JWT_SECRET=any-long-random-string
#   OPENAI_API_KEY=sk-...   (for embeddings + AI features)

# 3. Start PostgreSQL + pgai worker
docker compose up -d

# 4. Apply database schema
npm run db:migrate

# 5. Set up pgai vectorizer (run once â€” idempotent)
npm run setup:pgai

# 6. Start development server
npm run dev
```

### Available Commands

| Command | What it does |
|---------|-------------|
| `npm run dev` | Build TypeScript + start server (watch mode) |
| `npm run build` | Compile TypeScript to `dist/` |
| `npm run start` | Run compiled server (production) |
| `npm run db:generate` | Generate new migration from schema changes |
| `npm run db:migrate` | Apply pending migrations |
| `npm run setup:pgai` | Install pgai extension + create vectorizer (idempotent) |
| `npm run migrate:data` | Migrate data from MongoDB to PostgreSQL (one-time) |

### Docker Commands

```bash
docker compose up -d                  # Start all services
docker compose up -d db               # Start DB only (no vectorizer)
docker compose logs -f vectorizer-worker  # Watch embedding logs
docker compose down                   # Stop all
docker compose down -v                # Stop + wipe all data (fresh start)
```

---

## 11. Configuration Reference

All configuration lives in environment variables and `src/config.ts`.

### Environment Variables

```bash
# â”€â”€ Required â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATABASE_URL=postgres://brainly:brainly_dev@localhost:5432/brainly
JWT_SECRET=your-secret-at-least-32-chars

# â”€â”€ Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PORT=5000
CORS_ORIGIN=http://localhost:5173
LOG_LEVEL=info                         # trace|debug|info|warn|error|fatal

# â”€â”€ Database Pool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_POOL_MAX=20                         # Max concurrent DB connections

# â”€â”€ Auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_CLIENT_ID=                      # Optional â€” enables Google OAuth

# â”€â”€ Enrichment API Keys (all optional â€” degrades gracefully) â”€â”€â”€â”€â”€
YOUTUBE_API_KEY=                       # YouTube Data API v3
GITHUB_TOKEN=                          # GitHub Personal Access Token
TWITTER_BEARER_TOKEN=                  # Twitter/X Bearer Token
INSTAGRAM_APP_ID=                      # Instagram Basic Display API

# â”€â”€ AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY=sk-...                  # Required for embeddings + OpenAI LLM
LLM_PROVIDER=openai                    # 'openai' or 'anthropic'
ANTHROPIC_API_KEY=                     # Required if LLM_PROVIDER=anthropic

# â”€â”€ Legacy (only for data migration) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MONGO_URI=mongodb+srv://...
```

### `src/config.ts` â€” Feature Flags

```typescript
{
  providers: {
    youtube: true, twitter: true, instagram: true,
    github: true, medium: true, notion: true
  },
  extractors: {
    enabled: true,
    pollIntervalMs: 30_000,   // 30 seconds between enrichment batches
    maxRetries: 3,
    retryDelayMs: 60_000,     // 1 minute before retrying a failed item
  }
}
```

---

## 12. Production Readiness Assessment

### What Is Production-Ready

| Component | Status |
|-----------|--------|
| Schema design | âœ… Normalized, proper indexes, FK cascades |
| Authentication | âœ… bcrypt, JWT, Google OAuth, all races fixed |
| Error handling | âœ… try-catch on every endpoint, no stack trace leaks |
| Rate limiting | âœ… Global + per-route |
| Security headers | âœ… Helmet + CORS |
| Input validation | âœ… Zod on auth, manual checks on content |
| Enrichment service | âœ… Atomic claims, per-user fairness, retry logic, crash recovery |
| Graceful shutdown | âœ… SIGTERM/SIGINT: stops enrichment, closes server, drains pool |
| Database migrations | âœ… Versioned in `drizzle/` folder |
| Race conditions | âœ… All fixed with insert-catch-23505 pattern |

### Known Gaps (Before Real Production)

| Gap | Impact | Fix |
|-----|--------|-----|
| No `GET /health` endpoint | Can't tell if server is alive without SSHing | Add health check endpoint |
| Enrichment in-process | Crash = enrichment stops; no visibility | Separate process or BullMQ worker |
| No horizontal scaling for enrichment | Multiple instances = N polling loops | BullMQ + Redis job queue |
| No metrics or tracing | Can't diagnose performance issues | OpenTelemetry + Prometheus |
| Errors not tracked | No alerting on 500s | Sentry or similar |
| `.env` for secrets | Risk of accidental commit | AWS Secrets Manager / Doppler |
| No database backups | Data loss on volume deletion | Automated pg_dump + WAL archiving |
| No body size limit | Large payload attack vector | `express.json({ limit: '1mb' })` |
| Vectorizer worker has no restart policy | Silent crash = no embeddings | `restart: unless-stopped` in compose |

### Scalability Ceiling

| User count | Status | Bottleneck |
|------------|--------|-----------|
| 1â€“100 | âœ… Handles fine | None |
| 100â€“1,000 | âš ï¸ Mostly fine | Enrichment poll inefficiency |
| 1,000â€“10,000 | âŒ Issues | Single process, no queue |
| 10,000+ | âŒ Not designed for | Full redesign of job system |

---

## 13. Roadmap

### Phase 0 â€” PostgreSQL Migration âœ… Complete

- Drizzle ORM schema + versioned migrations
- All API routes migrated from Mongoose to Drizzle
- pgai vectorizer configured for `contents.full_text`
- Vercel AI SDK LLM client scaffolded
- Enrichment service producing `full_text` data
- Schema has `summary` + `ai_tags` columns ready for Phase 1

### Phase 1 â€” Search + Summarization + Auto-tags ğŸ”²

- `SemanticSearch` module â€” vector similarity via pgai
- `Summarizer` module â€” generates summary on content enrichment
- `AutoTagger` module â€” LLM-suggested tags from content analysis
- `GET /api/v1/search?q=<query>` endpoint
- Update enrichment pipeline to trigger summarization + tagging
- Frontend: search bar, summary on cards, tag suggestions UI

### Phase 2 â€” RAG Chat ğŸ”²

- `RAGChat` module composing SemanticSearch + Vercel AI SDK
- `conversations` + `messages` tables
- Streaming chat endpoint (SSE)
- Chat UI with conversation list, message thread, source citations

### Phase 3 â€” Agentic AI ğŸ”² (Requires separate design session)

- Agent framework architecture
- Tool definitions: web search, content analysis, knowledge graph
- Research agent, curation agent, learning agent
- **This phase will be designed separately before implementation**
